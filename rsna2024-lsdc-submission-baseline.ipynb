{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd534d0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.009985,
     "end_time": "2024-10-07T02:05:00.151003",
     "exception": false,
     "start_time": "2024-10-07T02:05:00.141018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Adopted from https://www.kaggle.com/code/yxyyxy/rsna2024-training-baseline-2nd-stage/edit\n",
    "\n",
    "# RSNA2024 LSDC Submission Baseline\n",
    "\n",
    "This notebook will Let the model infer and make a submission.\n",
    "\n",
    "### My other Notebooks\n",
    "- [RSNA2024 LSDC Making Dataset](https://www.kaggle.com/code/itsuki9180/rsna2024-lsdc-making-dataset) \n",
    "- [RSNA2024 LSDC Training Baseline](https://www.kaggle.com/code/itsuki9180/rsna2024-lsdc-training-baseline) \n",
    "- [RSNA2024 LSDC Submission Baseline](https://www.kaggle.com/code/itsuki9180/rsna2024-lsdc-submission-baseline) <- you're reading now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d64b4a",
   "metadata": {
    "papermill": {
     "duration": 0.009143,
     "end_time": "2024-10-07T02:05:00.169698",
     "exception": false,
     "start_time": "2024-10-07T02:05:00.160555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libralies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455e5bba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:00.189868Z",
     "iopub.status.busy": "2024-10-07T02:05:00.189437Z",
     "iopub.status.idle": "2024-10-07T02:05:00.202781Z",
     "shell.execute_reply": "2024-10-07T02:05:00.201755Z"
    },
    "papermill": {
     "duration": 0.026516,
     "end_time": "2024-10-07T02:05:00.205474",
     "exception": false,
     "start_time": "2024-10-07T02:05:00.178958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "if DEBUG == True:\n",
    "    rd = '/kaggle/input/rsna-lsdc-2024-submission-debug-dataset/debug'\n",
    "else:\n",
    "    rd = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification'\n",
    "    \n",
    "# Define the directory path\n",
    "DATA_fromStage1 = '/kaggle/working'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ac6485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:00.227189Z",
     "iopub.status.busy": "2024-10-07T02:05:00.226073Z",
     "iopub.status.idle": "2024-10-07T02:05:42.481847Z",
     "shell.execute_reply": "2024-10-07T02:05:42.480708Z"
    },
    "papermill": {
     "duration": 42.269627,
     "end_time": "2024-10-07T02:05:42.484775",
     "exception": false,
     "start_time": "2024-10-07T02:05:00.215148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using path: /kaggle/input/2d-segmentation-of-sagittal-lumbar-spine-mri/simple_unet.pth\r\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\r\n",
      "  warnings.warn(msg)\r\n",
      "Using path: /kaggle/input/rsna-lsdc-2024-submission-debug-dataset/debug/test_series_descriptions.csv\r\n",
      "Using base path: /kaggle/input/rsna-lsdc-2024-submission-debug-dataset/debug/test_images\r\n",
      "Processing studies: 100%|█████████████████████████| 6/6 [00:21<00:00,  3.55s/it]\r\n",
      "Pipeline completed successfully!\r\n",
      "Processing studies:  86%|█████████████████████▍   | 6/7 [00:07<00:01,  1.06s/it]Study 3429409220 was skipped due to error: 3429409220\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/kaggle/input/script-deepspine-custom-dataset/main.py\", line 662, in create_axial_dataset\r\n",
      "    sagittal_slice = sag_middle(sagittal_series_paths_T2[study_id][0])\r\n",
      "KeyError: 3429409220\r\n",
      "\r\n",
      "Processing studies: 100%|█████████████████████████| 7/7 [00:07<00:00,  1.01s/it]\r\n",
      "Metadata saved to /kaggle/working/axialT2/dataset_metadata.csv\r\n",
      "Axial dataset generation completed successfully!\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/input/script-deepspine-custom-dataset/main.py \\\n",
    "\"{rd}/test_series_descriptions.csv\" \\\n",
    "\"{rd}/sample_submission.csv\" \\\n",
    "\"{rd}/test_images\" \\\n",
    "'/kaggle/input/2d-segmentation-of-sagittal-lumbar-spine-mri/simple_unet.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aebab5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:42.509108Z",
     "iopub.status.busy": "2024-10-07T02:05:42.508085Z",
     "iopub.status.idle": "2024-10-07T02:05:48.469639Z",
     "shell.execute_reply": "2024-10-07T02:05:48.468620Z"
    },
    "papermill": {
     "duration": 5.976734,
     "end_time": "2024-10-07T02:05:48.472503",
     "exception": false,
     "start_time": "2024-10-07T02:05:42.495769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import timm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import albumentations as A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def3b52",
   "metadata": {
    "papermill": {
     "duration": 0.011526,
     "end_time": "2024-10-07T02:05:48.495196",
     "exception": false,
     "start_time": "2024-10-07T02:05:48.483670",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e45d0c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:48.519257Z",
     "iopub.status.busy": "2024-10-07T02:05:48.518569Z",
     "iopub.status.idle": "2024-10-07T02:05:48.593207Z",
     "shell.execute_reply": "2024-10-07T02:05:48.592171Z"
    },
    "papermill": {
     "duration": 0.089996,
     "end_time": "2024-10-07T02:05:48.595849",
     "exception": false,
     "start_time": "2024-10-07T02:05:48.505853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "N_WORKERS = os.cpu_count()\n",
    "USE_AMP = True\n",
    "SEED = 1\n",
    "\n",
    "IMG_SIZE = [224, 224]\n",
    "N_LABELS = 5\n",
    "N_CLASSES = 3 * N_LABELS\n",
    "\n",
    "model_name_sag = 'efficientnet_b0'\n",
    "model_name_axi = 'resnet34'\n",
    "# resnet34\n",
    "in_chans_sag = 30\n",
    "in_chans_axi = 4\n",
    "\n",
    "N_FOLDS = 5\n",
    "\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a347e39c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:48.620609Z",
     "iopub.status.busy": "2024-10-07T02:05:48.619429Z",
     "iopub.status.idle": "2024-10-07T02:05:48.628326Z",
     "shell.execute_reply": "2024-10-07T02:05:48.627154Z"
    },
    "papermill": {
     "duration": 0.023923,
     "end_time": "2024-10-07T02:05:48.630896",
     "exception": false,
     "start_time": "2024-10-07T02:05:48.606973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2dcaab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:48.655164Z",
     "iopub.status.busy": "2024-10-07T02:05:48.654723Z",
     "iopub.status.idle": "2024-10-07T02:05:48.660929Z",
     "shell.execute_reply": "2024-10-07T02:05:48.659838Z"
    },
    "papermill": {
     "duration": 0.021362,
     "end_time": "2024-10-07T02:05:48.663426",
     "exception": false,
     "start_time": "2024-10-07T02:05:48.642064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONDITIONS = [\n",
    "    'spinal_canal_stenosis', \n",
    "    'left_neural_foraminal_narrowing', \n",
    "    'right_neural_foraminal_narrowing',\n",
    "    'left_subarticular_stenosis',\n",
    "    'right_subarticular_stenosis'\n",
    "]\n",
    "\n",
    "# Define the mapping for each level\n",
    "level_mapping = {\n",
    "    'L1/L2': 'l1_l2',\n",
    "    'L2/L3': 'l2_l3',\n",
    "    'L3/L4': 'l3_l4',\n",
    "    'L4/L5': 'l4_l5',\n",
    "    'L5/S1': 'l5_s1'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb870f22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:48.687139Z",
     "iopub.status.busy": "2024-10-07T02:05:48.686689Z",
     "iopub.status.idle": "2024-10-07T02:05:48.692572Z",
     "shell.execute_reply": "2024-10-07T02:05:48.691522Z"
    },
    "papermill": {
     "duration": 0.020537,
     "end_time": "2024-10-07T02:05:48.695080",
     "exception": false,
     "start_time": "2024-10-07T02:05:48.674543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52412ba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:48.718590Z",
     "iopub.status.busy": "2024-10-07T02:05:48.718174Z",
     "iopub.status.idle": "2024-10-07T02:05:48.745938Z",
     "shell.execute_reply": "2024-10-07T02:05:48.744641Z"
    },
    "papermill": {
     "duration": 0.043124,
     "end_time": "2024-10-07T02:05:48.748906",
     "exception": false,
     "start_time": "2024-10-07T02:05:48.705782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sagittal T2 Metadata:\n",
      "                                                      L4/L5  \\\n",
      "st_id                                                         \n",
      "1176954132  /kaggle/working/sagittalT2/1176954132/L4_L5.npy   \n",
      "1737682527  /kaggle/working/sagittalT2/1737682527/L4_L5.npy   \n",
      "1891482189  /kaggle/working/sagittalT2/1891482189/L4_L5.npy   \n",
      "1972129014  /kaggle/working/sagittalT2/1972129014/L4_L5.npy   \n",
      "2676098721  /kaggle/working/sagittalT2/2676098721/L4_L5.npy   \n",
      "3004806533  /kaggle/working/sagittalT2/3004806533/L4_L5.npy   \n",
      "3429409220     /kaggle/working/axialT2/3429409220/L4_L5.npy   \n",
      "\n",
      "                                                      L3/L4  \\\n",
      "st_id                                                         \n",
      "1176954132  /kaggle/working/sagittalT2/1176954132/L3_L4.npy   \n",
      "1737682527  /kaggle/working/sagittalT2/1737682527/L3_L4.npy   \n",
      "1891482189  /kaggle/working/sagittalT2/1891482189/L3_L4.npy   \n",
      "1972129014  /kaggle/working/sagittalT2/1972129014/L3_L4.npy   \n",
      "2676098721  /kaggle/working/sagittalT2/2676098721/L3_L4.npy   \n",
      "3004806533  /kaggle/working/sagittalT2/3004806533/L3_L4.npy   \n",
      "3429409220     /kaggle/working/axialT2/3429409220/L3_L4.npy   \n",
      "\n",
      "                                                      L2/L3  \\\n",
      "st_id                                                         \n",
      "1176954132  /kaggle/working/sagittalT2/1176954132/L2_L3.npy   \n",
      "1737682527  /kaggle/working/sagittalT2/1737682527/L2_L3.npy   \n",
      "1891482189  /kaggle/working/sagittalT2/1891482189/L2_L3.npy   \n",
      "1972129014  /kaggle/working/sagittalT2/1972129014/L2_L3.npy   \n",
      "2676098721  /kaggle/working/sagittalT2/2676098721/L2_L3.npy   \n",
      "3004806533  /kaggle/working/sagittalT2/3004806533/L2_L3.npy   \n",
      "3429409220     /kaggle/working/axialT2/3429409220/L2_L3.npy   \n",
      "\n",
      "                                                      L1/L2  \\\n",
      "st_id                                                         \n",
      "1176954132  /kaggle/working/sagittalT2/1176954132/L1_L2.npy   \n",
      "1737682527  /kaggle/working/sagittalT2/1737682527/L1_L2.npy   \n",
      "1891482189  /kaggle/working/sagittalT2/1891482189/L1_L2.npy   \n",
      "1972129014  /kaggle/working/sagittalT2/1972129014/L1_L2.npy   \n",
      "2676098721  /kaggle/working/sagittalT2/2676098721/L1_L2.npy   \n",
      "3004806533  /kaggle/working/sagittalT2/3004806533/L1_L2.npy   \n",
      "3429409220     /kaggle/working/axialT2/3429409220/L1_L2.npy   \n",
      "\n",
      "                                                      L5/S1  \n",
      "st_id                                                        \n",
      "1176954132  /kaggle/working/sagittalT2/1176954132/L5_S1.npy  \n",
      "1737682527  /kaggle/working/sagittalT2/1737682527/L5_S1.npy  \n",
      "1891482189  /kaggle/working/sagittalT2/1891482189/L5_S1.npy  \n",
      "1972129014  /kaggle/working/sagittalT2/1972129014/L5_S1.npy  \n",
      "2676098721  /kaggle/working/sagittalT2/2676098721/L5_S1.npy  \n",
      "3004806533  /kaggle/working/sagittalT2/3004806533/L5_S1.npy  \n",
      "3429409220     /kaggle/working/axialT2/3429409220/L5_S1.npy  \n"
     ]
    }
   ],
   "source": [
    "# Define the paths to the CSV files\n",
    "csv_file_path_T2 = DATA_fromStage1 + '/sagittalT2/dataset_metadata.csv'\n",
    "csv_file_path_T1 = DATA_fromStage1 + '/sagittalT1/dataset_metadata.csv'\n",
    "csv_file_path_axial = DATA_fromStage1 + '/axialT2/dataset_metadata.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "dataset_metadata_T2 = pd.read_csv(csv_file_path_T2)\n",
    "dataset_metadata_T1 = pd.read_csv(csv_file_path_T1)\n",
    "dataset_metadata_axial = pd.read_csv(csv_file_path_axial)\n",
    "\n",
    "\n",
    "# Rename columns and set index\n",
    "dataset_metadata_T2.rename(columns={'Unnamed: 0': 'st_id'}, inplace=True)\n",
    "dataset_metadata_T2.set_index('st_id', inplace=True)\n",
    "\n",
    "dataset_metadata_T1.rename(columns={'Unnamed: 0': 'st_id'}, inplace=True)\n",
    "dataset_metadata_T1.set_index('st_id', inplace=True)\n",
    "\n",
    "dataset_metadata_axial.rename(columns={'Unnamed: 0': 'st_id'}, inplace=True)\n",
    "dataset_metadata_axial.set_index('st_id', inplace=True)\n",
    "\n",
    "missing_st_id_axial = dataset_metadata_axial.index.difference(dataset_metadata_T2.index)\n",
    "missing_metadata_axial = dataset_metadata_axial.loc[missing_st_id_axial]\n",
    "# Concatenate the missing metadata to dataset_metadata_T2\n",
    "dataset_metadata_T2 = pd.concat([dataset_metadata_T2, missing_metadata_axial])\n",
    "\n",
    "# Print the DataFrames\n",
    "print(\"Sagittal T2 Metadata:\")\n",
    "print(dataset_metadata_T2)\n",
    "\n",
    "# print(\"\\nSagittal T1 Metadata:\")\n",
    "# print(dataset_metadata_T1)\n",
    "\n",
    "# print(\"\\nAxial T2 Metadata:\")\n",
    "# print(dataset_metadata_axial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf75cc10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:48.775779Z",
     "iopub.status.busy": "2024-10-07T02:05:48.774893Z",
     "iopub.status.idle": "2024-10-07T02:05:48.784636Z",
     "shell.execute_reply": "2024-10-07T02:05:48.783420Z"
    },
    "papermill": {
     "duration": 0.025527,
     "end_time": "2024-10-07T02:05:48.787397",
     "exception": false,
     "start_time": "2024-10-07T02:05:48.761870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# one npy file example\\nprint(\"--------------------- SagittalT2 samples -----------\")\\nexample_npy_fn = dataset_metadata[\\'L4/L5\\'][0]\\nexample_npy_fn = example_npy_fn.split(\\'/\\')\\n# Load the .npy file\\nexample_npy = np.load(os.path.join(DATA_fromStage1, *(example_npy_fn[-3:])))\\n# Print the structure of the data\\nprint(f\"Shape of the data: {example_npy.shape}\")\\nprint(f\"Data type: {example_npy.dtype}\")\\n# Show the 15 slices one by one\\nfor i in range(15):\\n    plt.imshow(example_npy[i], cmap=\\'gray\\')\\n    plt.title(f\\'Slice {i+1}\\')\\n    plt.axis(\\'off\\')\\n    plt.show()\\n\\nprint(\"--------------------- SagittalT1 samples -----------\")\\nexample_npy = np.load(os.path.join(DATA_fromStage1, \\'sagittalT1\\', *(example_npy_fn[-2:])))\\n# Print the structure of the data\\nprint(f\"Shape of the data: {example_npy.shape}\")\\nprint(f\"Data type: {example_npy.dtype}\")\\n# Show the 15 slices one by one\\nfor i in range(15):\\n    plt.imshow(example_npy[i], cmap=\\'gray\\')\\n    plt.title(f\\'Slice {i+1}\\')\\n    plt.axis(\\'off\\')\\n    plt.show()\\n\\nprint(\"--------------------- AxialT2 samples ----------\")\\n# Load the .npy file\\nexample_npy = np.load(os.path.join(DATA_fromStage1, \\'axialT2\\', *(example_npy_fn[-2:])))\\nprint(f\"Shape of the data: {example_npy.shape}\")\\nprint(f\"Data type: {example_npy.dtype}\")\\nfor i in range(6):\\n    img = example_npy[i]\\n    plt.imshow(img, cmap=\\'gray\\')\\n    plt.title(f\\'Slice {i+1}\\')\\n    plt.axis(\\'off\\')\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# one npy file example\n",
    "print(\"--------------------- SagittalT2 samples -----------\")\n",
    "example_npy_fn = dataset_metadata['L4/L5'][0]\n",
    "example_npy_fn = example_npy_fn.split('/')\n",
    "# Load the .npy file\n",
    "example_npy = np.load(os.path.join(DATA_fromStage1, *(example_npy_fn[-3:])))\n",
    "# Print the structure of the data\n",
    "print(f\"Shape of the data: {example_npy.shape}\")\n",
    "print(f\"Data type: {example_npy.dtype}\")\n",
    "# Show the 15 slices one by one\n",
    "for i in range(15):\n",
    "    plt.imshow(example_npy[i], cmap='gray')\n",
    "    plt.title(f'Slice {i+1}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"--------------------- SagittalT1 samples -----------\")\n",
    "example_npy = np.load(os.path.join(DATA_fromStage1, 'sagittalT1', *(example_npy_fn[-2:])))\n",
    "# Print the structure of the data\n",
    "print(f\"Shape of the data: {example_npy.shape}\")\n",
    "print(f\"Data type: {example_npy.dtype}\")\n",
    "# Show the 15 slices one by one\n",
    "for i in range(15):\n",
    "    plt.imshow(example_npy[i], cmap='gray')\n",
    "    plt.title(f'Slice {i+1}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"--------------------- AxialT2 samples ----------\")\n",
    "# Load the .npy file\n",
    "example_npy = np.load(os.path.join(DATA_fromStage1, 'axialT2', *(example_npy_fn[-2:])))\n",
    "print(f\"Shape of the data: {example_npy.shape}\")\n",
    "print(f\"Data type: {example_npy.dtype}\")\n",
    "for i in range(6):\n",
    "    img = example_npy[i]\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f'Slice {i+1}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f56ee1",
   "metadata": {
    "papermill": {
     "duration": 0.011599,
     "end_time": "2024-10-07T02:05:48.811390",
     "exception": false,
     "start_time": "2024-10-07T02:05:48.799791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af6e3195",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:48.837696Z",
     "iopub.status.busy": "2024-10-07T02:05:48.836590Z",
     "iopub.status.idle": "2024-10-07T02:05:48.862793Z",
     "shell.execute_reply": "2024-10-07T02:05:48.861964Z"
    },
    "papermill": {
     "duration": 0.042339,
     "end_time": "2024-10-07T02:05:48.865596",
     "exception": false,
     "start_time": "2024-10-07T02:05:48.823257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RSNA24TestDataset(Dataset):\n",
    "    def __init__(self, df_fn, Slice_len_Sag=15, Slice_len_Axi=4, transform=None, trainsform_axis=None):\n",
    "        self.df_fn = df_fn\n",
    "        self.transform = transform\n",
    "        self.trainsform_axis = trainsform_axis\n",
    "        self.Slice_len_Sag = Slice_len_Sag\n",
    "        self.Slice_len_Axi = Slice_len_Axi\n",
    "        # Select all rows where the 'Name' column has the value 'Alice'\n",
    "    def __len__(self):\n",
    "        return len(self.df_fn)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        st_id = self.df_fn.index[idx]\n",
    "        row_idx = self.df_fn.iloc[idx]\n",
    "        levels = ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']\n",
    "        filenames = [row_idx[level] for level in levels]\n",
    "        \n",
    "        npy_sagT2_list = []\n",
    "        npy_sagT1_list = []\n",
    "        npy_AxiT2_list = []\n",
    "    \n",
    "        for filename in filenames:\n",
    "            path_split = filename.split('/')\n",
    "            # Saggital T2 ------------  Load the .npy file\n",
    "            try:\n",
    "                npy_sagT2_path = os.path.join(DATA_fromStage1, \"sagittalT2\", path_split[4], path_split[5])\n",
    "                npy_sagT2 = np.load(npy_sagT2_path).astype(np.float32)\n",
    "                current_length = npy_sagT2.shape[0]\n",
    "                if current_length > self.Slice_len_Sag:\n",
    "                    indices = np.linspace(0, current_length - 1, self.Slice_len_Sag, dtype=int)\n",
    "                    npy_sagT2 = npy_sagT2[indices, :, :]\n",
    "                elif current_length < self.Slice_len_Sag:\n",
    "                    indices = np.linspace(0, current_length - 1, self.Slice_len_Sag, dtype=int)\n",
    "                    npy_sagT2 = npy_sagT2[indices, :, :]\n",
    "            except FileNotFoundError:\n",
    "                # If the file does not exist, append an empty array\n",
    "                npy_sagT2 = np.zeros((self.Slice_len_Sag, 84, 160), dtype=np.float32)  # Adjust the shape as needed\n",
    "            npy_sagT2_list.append(npy_sagT2)\n",
    "            # Saggital T1 ------------  Load the .npy file\n",
    "            try:\n",
    "                npy_sagT1_path = os.path.join(DATA_fromStage1, \"sagittalT1\", path_split[4], path_split[5])\n",
    "                npy_sagT1 = np.load(npy_sagT1_path).astype(np.float32)\n",
    "                current_length = npy_sagT1.shape[0]\n",
    "                if current_length > self.Slice_len_Sag:\n",
    "                    indices = np.linspace(0, current_length - 1, self.Slice_len_Sag, dtype=int)\n",
    "                    npy_sagT1 = npy_sagT1[indices, :, :]\n",
    "                elif current_length < self.Slice_len_Sag:\n",
    "                    indices = np.linspace(0, current_length - 1, self.Slice_len_Sag, dtype=int)\n",
    "                    npy_sagT1 = npy_sagT1[indices, :, :]\n",
    "            except FileNotFoundError:\n",
    "                # If the file does not exist, append an empty array\n",
    "                npy_sagT1 = np.zeros((self.Slice_len_Sag, 84, 160), dtype=np.float32)  # Adjust the shape as needed\n",
    "            npy_sagT1_list.append(npy_sagT1)\n",
    "            # Axial T2 ------------ Load the .npy file\n",
    "            try:\n",
    "                npy_AxiT2_path = os.path.join(DATA_fromStage1 , \"axialT2\", path_split[4], path_split[5])\n",
    "                npy_AxiT2 = np.load(npy_AxiT2_path).astype(np.float32)\n",
    "                current_length = npy_AxiT2.shape[0]\n",
    "                if current_length > self.Slice_len_Axi:\n",
    "                    indices = np.linspace(0, current_length - 1, self.Slice_len_Axi, dtype=int)\n",
    "                    npy_AxiT2 = npy_AxiT2[indices, :, :]\n",
    "                elif current_length < self.Slice_len_Axi:\n",
    "                    indices = np.linspace(0, current_length - 1, self.Slice_len_Axi, dtype=int)\n",
    "                    npy_AxiT2 = npy_AxiT2[indices, :, :]\n",
    "            except FileNotFoundError:\n",
    "                # If the file does not exist, append an empty array\n",
    "                npy_AxiT2 = np.zeros((self.Slice_len_Sag, 224, 224), dtype=np.float32)  # Adjust the shape as needed\n",
    "            npy_AxiT2_list.append(npy_AxiT2)\n",
    "            \n",
    "        # Transpose and transform the data\n",
    "        npy_sagT1_list = [np.transpose(npy, (1, 2, 0)) for npy in npy_sagT1_list]\n",
    "        npy_sagT2_list = [np.transpose(npy, (1, 2, 0)) for npy in npy_sagT2_list]\n",
    "        npy_AxiT2_list = [np.transpose(npy, (1, 2, 0)) for npy in npy_AxiT2_list]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            npy_sagT1_list = [self.transform(image=npy)['image'] for npy in npy_sagT1_list]\n",
    "            npy_sagT2_list = [self.transform(image=npy)['image'] for npy in npy_sagT2_list]\n",
    "            npy_AxiT2_list = [self.trainsform_axis(image=npy)['image'] for npy in npy_AxiT2_list]\n",
    "\n",
    "        # Transpose back to the original format\n",
    "        npy_sagT1_list = [np.transpose(npy, (2, 0, 1)) for npy in npy_sagT1_list]\n",
    "        npy_sagT2_list = [np.transpose(npy, (2, 0, 1)) for npy in npy_sagT2_list]\n",
    "        npy_AxiT2_list = [np.transpose(npy, (2, 0, 1)) for npy in npy_AxiT2_list]\n",
    "\n",
    "        return st_id, npy_sagT1_list, npy_sagT2_list, npy_AxiT2_list, levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56ae8008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:48.891047Z",
     "iopub.status.busy": "2024-10-07T02:05:48.889980Z",
     "iopub.status.idle": "2024-10-07T02:05:48.896821Z",
     "shell.execute_reply": "2024-10-07T02:05:48.895797Z"
    },
    "papermill": {
     "duration": 0.022127,
     "end_time": "2024-10-07T02:05:48.899384",
     "exception": false,
     "start_time": "2024-10-07T02:05:48.877257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms_test_Sag = A.Compose([\n",
    "    A.Resize(84, 160),\n",
    "    A.Normalize(mean=0.5, std=0.5)\n",
    "])\n",
    "\n",
    "transforms_test = A.Compose([\n",
    "    A.Resize(IMG_SIZE[0], IMG_SIZE[1]),\n",
    "    A.Normalize(mean=0.5, std=0.5)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b7495e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:48.923671Z",
     "iopub.status.busy": "2024-10-07T02:05:48.923273Z",
     "iopub.status.idle": "2024-10-07T02:05:48.929000Z",
     "shell.execute_reply": "2024-10-07T02:05:48.928011Z"
    },
    "papermill": {
     "duration": 0.020475,
     "end_time": "2024-10-07T02:05:48.931321",
     "exception": false,
     "start_time": "2024-10-07T02:05:48.910846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ds = RSNA24TestDataset(dataset_metadata_T2, transform=transforms_test_Sag, trainsform_axis=transforms_test)\n",
    "test_dl = DataLoader(\n",
    "    test_ds, \n",
    "    batch_size=1, \n",
    "    shuffle=False,\n",
    "    num_workers=N_WORKERS,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b57bc6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:48.956591Z",
     "iopub.status.busy": "2024-10-07T02:05:48.956176Z",
     "iopub.status.idle": "2024-10-07T02:05:48.964145Z",
     "shell.execute_reply": "2024-10-07T02:05:48.963103Z"
    },
    "papermill": {
     "duration": 0.023229,
     "end_time": "2024-10-07T02:05:48.966496",
     "exception": false,
     "start_time": "2024-10-07T02:05:48.943267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from collections import Counter\\n\\nsagT2_slicenum = []\\nAxiT2_slicenum = []\\n\\nprint(test_dl.__len__())\\n\\n# Iterate through the data loader and append slice numbers\\nfor idx, (st_id, npy_sagT1_list, npy_sagT2_list, npy_AxiT2_list, levels) in enumerate(test_dl):\\n    for npy_sagT2 in npy_sagT2_list:\\n        sagT2_slicenum.append(npy_sagT2.shape)\\n        assert not torch.isnan(npy_sagT2).any(), \"NaN values found in npy_sagT2\"\\n    \\n    for npy_AxiT2 in npy_AxiT2_list:\\n        AxiT2_slicenum.append(npy_AxiT2.shape)\\n        assert not torch.isnan(npy_AxiT2).any(), \"NaN values found in npy_AxiT2\"\\n    \\n    print(f\"Batch {idx + 1}: Levels - {levels}\")\\n\\n# Count the occurrences of each unique value in the lists\\nsagT2_counts = Counter(sagT2_slicenum)\\nAxiT2_counts = Counter(AxiT2_slicenum)\\n\\nprint(\"Occurrences of each unique value in sagT2_slicenum:\")\\nfor value, count in sagT2_counts.items():\\n    print(f\"Value: {value}, Count: {count}\")\\n\\nprint(\"Occurrences of each unique value in AxiT2_slicenum:\")\\nfor value, count in AxiT2_counts.items():\\n    print(f\"Value: {value}, Count: {count}\")'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from collections import Counter\n",
    "\n",
    "sagT2_slicenum = []\n",
    "AxiT2_slicenum = []\n",
    "\n",
    "print(test_dl.__len__())\n",
    "\n",
    "# Iterate through the data loader and append slice numbers\n",
    "for idx, (st_id, npy_sagT1_list, npy_sagT2_list, npy_AxiT2_list, levels) in enumerate(test_dl):\n",
    "    for npy_sagT2 in npy_sagT2_list:\n",
    "        sagT2_slicenum.append(npy_sagT2.shape)\n",
    "        assert not torch.isnan(npy_sagT2).any(), \"NaN values found in npy_sagT2\"\n",
    "    \n",
    "    for npy_AxiT2 in npy_AxiT2_list:\n",
    "        AxiT2_slicenum.append(npy_AxiT2.shape)\n",
    "        assert not torch.isnan(npy_AxiT2).any(), \"NaN values found in npy_AxiT2\"\n",
    "    \n",
    "    print(f\"Batch {idx + 1}: Levels - {levels}\")\n",
    "\n",
    "# Count the occurrences of each unique value in the lists\n",
    "sagT2_counts = Counter(sagT2_slicenum)\n",
    "AxiT2_counts = Counter(AxiT2_slicenum)\n",
    "\n",
    "print(\"Occurrences of each unique value in sagT2_slicenum:\")\n",
    "for value, count in sagT2_counts.items():\n",
    "    print(f\"Value: {value}, Count: {count}\")\n",
    "\n",
    "print(\"Occurrences of each unique value in AxiT2_slicenum:\")\n",
    "for value, count in AxiT2_counts.items():\n",
    "    print(f\"Value: {value}, Count: {count}\")'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd68c4",
   "metadata": {
    "papermill": {
     "duration": 0.011571,
     "end_time": "2024-10-07T02:05:48.989854",
     "exception": false,
     "start_time": "2024-10-07T02:05:48.978283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e62173b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:49.015958Z",
     "iopub.status.busy": "2024-10-07T02:05:49.014929Z",
     "iopub.status.idle": "2024-10-07T02:05:49.031935Z",
     "shell.execute_reply": "2024-10-07T02:05:49.030797Z"
    },
    "papermill": {
     "duration": 0.032757,
     "end_time": "2024-10-07T02:05:49.034373",
     "exception": false,
     "start_time": "2024-10-07T02:05:49.001616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LevelHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LevelHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class RSNA24Model_Hybrid(nn.Module):\n",
    "    def __init__(self, model_name_sag, model_name_axi, in_chans_sag, in_chans_axi, num_classes, level_names):\n",
    "        super(RSNA24Model_Hybrid, self).__init__()\n",
    "        self.model_sag = timm.create_model(model_name_sag, in_chans=in_chans_sag, global_pool='avg'\n",
    "                                           , pretrained=False, features_only=False)\n",
    "        self.model_axi = timm.create_model(model_name_axi, in_chans=in_chans_axi, global_pool='avg'\n",
    "                                           , pretrained=False, features_only=False)\n",
    "        \n",
    "        # Replace the last layer with an identity layer\n",
    "        if hasattr(self.model_sag, 'classifier'):\n",
    "            self.model_sag.classifier = nn.Identity()\n",
    "        elif hasattr(self.model_sag, 'fc'):\n",
    "            self.model_sag.fc = nn.Identity()\n",
    "        \n",
    "        if hasattr(self.model_axi, 'classifier'):\n",
    "            self.model_axi.classifier = nn.Identity()\n",
    "        elif hasattr(self.model_axi, 'fc'):\n",
    "            self.model_axi.fc = nn.Identity()\n",
    "        \n",
    "        # Get the output feature sizes\n",
    "        with torch.no_grad():\n",
    "            sample_input_sag = torch.randn(1, in_chans_sag, 84, 160)\n",
    "            sample_input_axi = torch.randn(1, in_chans_axi, 224, 224)\n",
    "            output_sag = self.model_sag(sample_input_sag)\n",
    "            output_axi = self.model_axi(sample_input_axi)\n",
    "        \n",
    "        # Define the final fully connected layers for each task\n",
    "        self.fc_heads = nn.ModuleDict({\n",
    "            level: LevelHead(output_sag.shape[1] + output_axi.shape[1], num_classes) for level in level_names\n",
    "        })\n",
    "        \n",
    "    def forward(self, x_sag, x_axi, level):\n",
    "        x_sag = self.model_sag(x_sag)\n",
    "        x_axi = self.model_axi(x_axi)\n",
    "        x = torch.cat((x_sag, x_axi), dim=1)\n",
    "        x = self.fc_heads[level[0]](x) # the input level is a array associated with batch size\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79523ccc",
   "metadata": {
    "papermill": {
     "duration": 0.011556,
     "end_time": "2024-10-07T02:05:49.057618",
     "exception": false,
     "start_time": "2024-10-07T02:05:49.046062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b85e6261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:49.082802Z",
     "iopub.status.busy": "2024-10-07T02:05:49.081818Z",
     "iopub.status.idle": "2024-10-07T02:05:49.094668Z",
     "shell.execute_reply": "2024-10-07T02:05:49.093687Z"
    },
    "papermill": {
     "duration": 0.027967,
     "end_time": "2024-10-07T02:05:49.097312",
     "exception": false,
     "start_time": "2024-10-07T02:05:49.069345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CKPT_PATHS = glob.glob('/kaggle/input/tpu-rsna2024-training-baseline-2nd-stage/rsna24-results/best_wll_model_fold-*.pt')\n",
    "CKPT_PATHS = sorted(CKPT_PATHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fec6d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:49.123142Z",
     "iopub.status.busy": "2024-10-07T02:05:49.122368Z",
     "iopub.status.idle": "2024-10-07T02:05:59.679522Z",
     "shell.execute_reply": "2024-10-07T02:05:59.678371Z"
    },
    "papermill": {
     "duration": 10.57261,
     "end_time": "2024-10-07T02:05:59.682060",
     "exception": false,
     "start_time": "2024-10-07T02:05:49.109450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /kaggle/input/tpu-rsna2024-training-baseline-2nd-stage/rsna24-results/best_wll_model_fold-0.pt...\n",
      "loading /kaggle/input/tpu-rsna2024-training-baseline-2nd-stage/rsna24-results/best_wll_model_fold-1.pt...\n",
      "loading /kaggle/input/tpu-rsna2024-training-baseline-2nd-stage/rsna24-results/best_wll_model_fold-2.pt...\n",
      "loading /kaggle/input/tpu-rsna2024-training-baseline-2nd-stage/rsna24-results/best_wll_model_fold-3.pt...\n",
      "loading /kaggle/input/tpu-rsna2024-training-baseline-2nd-stage/rsna24-results/best_wll_model_fold-4.pt...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print(models)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure device is set correctly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "models = []\n",
    "for i, cp in enumerate(CKPT_PATHS):\n",
    "    print(f'loading {cp}...')\n",
    "    model = RSNA24Model_Hybrid(model_name_sag, model_name_axi,\n",
    "                               in_chans_sag, in_chans_axi, \n",
    "                               num_classes=N_CLASSES, level_names=level_mapping.keys())\n",
    "    model.load_state_dict(torch.load(cp, map_location=device))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    models.append(model)\n",
    "\n",
    "'''print(models)'''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdffd49",
   "metadata": {
    "papermill": {
     "duration": 0.011854,
     "end_time": "2024-10-07T02:05:59.706101",
     "exception": false,
     "start_time": "2024-10-07T02:05:59.694247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6da65d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:05:59.732256Z",
     "iopub.status.busy": "2024-10-07T02:05:59.731451Z",
     "iopub.status.idle": "2024-10-07T02:06:05.102088Z",
     "shell.execute_reply": "2024-10-07T02:06:05.100915Z"
    },
    "papermill": {
     "duration": 5.38819,
     "end_time": "2024-10-07T02:06:05.106099",
     "exception": false,
     "start_time": "2024-10-07T02:05:59.717909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:05<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n",
      "175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "autocast = torch.cuda.amp.autocast(enabled=USE_AMP, dtype=torch.half)\n",
    "y_preds = []\n",
    "row_names = []\n",
    "\n",
    "seq_cond = [1, 3, 2, 4, 0]\n",
    "with tqdm(test_dl, leave=True) as pbar:\n",
    "    with torch.no_grad():\n",
    "        for idx, (st_id, npy_sagT1_list, npy_sagT2_list, npy_AxiT2_list, levels) in enumerate(pbar):\n",
    "            pred_per_study = np.zeros((25, 3))\n",
    "            index = 0  # Initialize the index counter\n",
    "            for npy_sagT1, npy_sagT2, npy_AxiT2, level in zip(npy_sagT1_list, npy_sagT2_list, npy_AxiT2_list, levels):        \n",
    "                pred_per_study_level = np.zeros((5, 3))  \n",
    "                npy_sagT1 = npy_sagT1.to(device)\n",
    "                npy_sagT2 = npy_sagT2.to(device)\n",
    "                npy_AxiT2 = npy_AxiT2.to(device)\n",
    "                with torch.cuda.amp.autocast(): \n",
    "                    for m in models:    \n",
    "                        y = m(torch.cat((npy_sagT1, npy_sagT2), axis=1), npy_AxiT2, level)[0]\n",
    "                        for col in range(N_LABELS):\n",
    "                            pred = y[col*3:col*3+3]\n",
    "                            y_pred = pred.float().softmax(0).cpu().numpy()\n",
    "                            pred_per_study_level[col] += y_pred / len(models)\n",
    "                    # pred_per_study_level (5, 3)\n",
    "                for i in range(5):\n",
    "                    pred_per_study[index + i*5, :] = pred_per_study_level[seq_cond[i], :]\n",
    "                index += 1  # Increment the index for the next iteration\n",
    "\n",
    "            # Add row names following the new sequence\n",
    "            for cond_idx in seq_cond:\n",
    "                cond = CONDITIONS[cond_idx]\n",
    "                for i in range(5):\n",
    "                    row_name = f\"{str(st_id.item())}_{cond}_{level_mapping[''.join(levels[i])]}\"\n",
    "                    row_names.append(row_name)\n",
    "            \n",
    "            y_preds.append(pred_per_study)\n",
    "        \n",
    "y_preds = np.concatenate(y_preds, axis=0)\n",
    "                    \n",
    "print(len(row_names))\n",
    "print(len(y_preds))\n",
    "\n",
    "CONDITIONS = [\n",
    "    'spinal_canal_stenosis', \n",
    "    'left_neural_foraminal_narrowing', \n",
    "    'right_neural_foraminal_narrowing',\n",
    "    'left_subarticular_stenosis',\n",
    "    'right_subarticular_stenosis'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8c7a585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:06:05.135731Z",
     "iopub.status.busy": "2024-10-07T02:06:05.134871Z",
     "iopub.status.idle": "2024-10-07T02:06:05.156254Z",
     "shell.execute_reply": "2024-10-07T02:06:05.155206Z"
    },
    "papermill": {
     "duration": 0.03968,
     "end_time": "2024-10-07T02:06:05.159310",
     "exception": false,
     "start_time": "2024-10-07T02:06:05.119630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(f'{rd}/sample_submission.csv')\n",
    "LABELS = list(sample_sub.columns[1:])\n",
    "# print(sample_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ae249",
   "metadata": {
    "papermill": {
     "duration": 0.013289,
     "end_time": "2024-10-07T02:06:05.187289",
     "exception": false,
     "start_time": "2024-10-07T02:06:05.174000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be6e01b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:06:05.215941Z",
     "iopub.status.busy": "2024-10-07T02:06:05.215475Z",
     "iopub.status.idle": "2024-10-07T02:06:05.227363Z",
     "shell.execute_reply": "2024-10-07T02:06:05.226323Z"
    },
    "papermill": {
     "duration": 0.029121,
     "end_time": "2024-10-07T02:06:05.229950",
     "exception": false,
     "start_time": "2024-10-07T02:06:05.200829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['row_id'] = row_names\n",
    "sub[LABELS] = y_preds\n",
    "# # print(sub.index)\n",
    "# print(sub)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7fd0e06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:06:05.259844Z",
     "iopub.status.busy": "2024-10-07T02:06:05.258594Z",
     "iopub.status.idle": "2024-10-07T02:06:05.450521Z",
     "shell.execute_reply": "2024-10-07T02:06:05.449239Z"
    },
    "papermill": {
     "duration": 0.209714,
     "end_time": "2024-10-07T02:06:05.452979",
     "exception": false,
     "start_time": "2024-10-07T02:06:05.243265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/3706240136.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  sample_sub.iloc[:, 1:] = sample_sub.iloc[:, 1:].applymap(lambda x: 0.00 if pd.isnull(x) or isinstance(x, (str)) else x)\n"
     ]
    }
   ],
   "source": [
    "for index, row in sample_sub.iterrows():\n",
    "    if row['row_id'] == sub.loc[index, 'row_id']:\n",
    "        sample_sub.loc[index] = sub.loc[index]\n",
    "\n",
    "# # Print the data types of each column\n",
    "# print(\"\\nData types of each column:\")\n",
    "# print(sample_sub.dtypes)\n",
    "\n",
    "# Replace incorrect data types and empty values with 0\n",
    "sample_sub.iloc[:, 1:] = sample_sub.iloc[:, 1:].applymap(lambda x: 0.00 if pd.isnull(x) or isinstance(x, (str)) else x)\n",
    "\n",
    "# sample_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb479846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:06:05.481031Z",
     "iopub.status.busy": "2024-10-07T02:06:05.480590Z",
     "iopub.status.idle": "2024-10-07T02:06:05.490124Z",
     "shell.execute_reply": "2024-10-07T02:06:05.489227Z"
    },
    "papermill": {
     "duration": 0.026287,
     "end_time": "2024-10-07T02:06:05.492450",
     "exception": false,
     "start_time": "2024-10-07T02:06:05.466163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_sub.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8561470,
     "sourceId": 71549,
     "sourceType": "competition"
    },
    {
     "datasetId": 5471909,
     "sourceId": 9529005,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 198313435,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 199192833,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 199735585,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 71.420873,
   "end_time": "2024-10-07T02:06:08.381513",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-07T02:04:56.960640",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
